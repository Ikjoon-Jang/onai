import os
import pickle
from dotenv import load_dotenv
from embedding.faiss_store import save_embeddings_to_faiss
from utils.ontology_to_text import ontology_elements_to_sentences
from fuseki.fuseki_query import get_all_ontology_elements
import faiss

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

FAISS_INDEX_FILE = os.getenv("FAISS_INDEX_FILE")
FAISS_META_FILE = os.getenv("FAISS_META_FILE")

# âœ… ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
if os.path.exists(FAISS_INDEX_FILE):
    os.remove(FAISS_INDEX_FILE)
    print(f"ğŸ§¹ ê¸°ì¡´ ì¸ë±ìŠ¤ íŒŒì¼ ì‚­ì œ: {FAISS_INDEX_FILE}")

if os.path.exists(FAISS_META_FILE):
    with open(FAISS_META_FILE, "rb") as f:
        old_data = pickle.load(f)

    cleaned = []
    for item in old_data:
        if isinstance(item, str):
            cleaned.append({"text": item, "source": "legacy"})
        elif isinstance(item, dict) and "text" in item:
            cleaned.append(item)

    with open(FAISS_META_FILE, "wb") as f:
        pickle.dump(cleaned, f)

    print(f"ğŸ§¹ ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {len(cleaned)}ê°œ í•­ëª© ìœ ì§€")

# 2. Fusekiì—ì„œ ì˜¨í†¨ë¡œì§€ ìš”ì†Œ ê°€ì ¸ì˜¤ê¸°
classes, object_props, data_props, individuals, rules = get_all_ontology_elements()

# 3. ìš”ì†Œë“¤ì„ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜ í›„ ë©”íƒ€ë°ì´í„° í¬ë§· í†µì¼
sentences = ontology_elements_to_sentences(classes, object_props, data_props, individuals, rules)
metadata = [{"text": s, "source": "ontology"} for s in sentences]

# 4. FAISS ì €ì¥
save_embeddings_to_faiss(sentences, metadata)

print(f"âœ… ì´ {len(sentences)}ê°œì˜ ë¬¸ì¥ì„ ì„ë² ë”©í•˜ê³  ì¸ë±ìŠ¤ë¥¼ ìƒˆë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
