import logging
from datetime import datetime
import os

from fuseki.fuseki_query import get_all_ontology_elements
from utils.ontology_to_text import ontology_elements_to_sentences_parallel
from embedding.embedding import embed_sentences
from embedding.faiss_store import save_embeddings_to_faiss

# ğŸ”§ ë¡œê·¸ ì„¤ì •
os.makedirs("logs", exist_ok=True)
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
log_file = f"logs/pipeline_{timestamp}.log"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.FileHandler(log_file, encoding="utf-8"), logging.StreamHandler()],
)

SENTENCE_LOG_FILE = f"logs/sentences_{timestamp}.log"

def save_sentences_to_file(sentences, filename=SENTENCE_LOG_FILE):
    with open(filename, "w", encoding="utf-8") as f:
        for i, s in enumerate(sentences, 1):
            f.write(f"{i}. {s}\n")
    logging.info(f"âœ… ìì—°ì–´ ë¬¸ì¥ {len(sentences)}ê°œë¥¼ '{filename}'ì— ì €ì¥ ì™„ë£Œ")

def main():
    logging.info("ğŸš€ Fusekië¡œë¶€í„° ì˜¨í†¨ë¡œì§€ ìš”ì†Œ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    data = get_all_ontology_elements()
    logging.info("âœ… ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ")

    logging.info("ğŸ“ ìì—°ì–´ ë¬¸ì¥ ë³€í™˜ (í•œê¸€+ì˜ë¬¸ ë³‘ë ¬) ì¤‘...")
    sentences = ontology_elements_to_sentences_parallel(
        data["classes"],
        data["object_props"],
        data["data_props"],
        data["individuals"],
        data["rules"]
    )
    logging.info(f"âœ… ë¬¸ì¥ ìˆ˜: {len(sentences)}")
    save_sentences_to_file(sentences)

    logging.info("ğŸ” OpenAI ì„ë² ë”© ì¤‘...")
    embeddings = embed_sentences(sentences)
    logging.info(f"âœ… ìœ íš¨ ì„ë² ë”© ìˆ˜: {len(embeddings)}")

    logging.info("ğŸ’¾ FAISS ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
    save_embeddings_to_faiss(sentences, embeddings)
    logging.info("ğŸ‰ ì™„ë£Œ! FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ")

if __name__ == "__main__":
    main()
