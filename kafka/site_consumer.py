from kafka import KafkaConsumer
import json
import os
from dotenv import load_dotenv
from openai import OpenAI
import logging
import faiss
import numpy as np
import pickle

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 2. OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# 3. ë¡œê·¸ ì„¤ì •
os.makedirs("logs", exist_ok=True)
logging.basicConfig(
    filename="logs/qlinx_site_consumer.log",
    level=logging.INFO,
    format="%(asctime)s - %(message)s"
)

# 4. FAISS ì¸ë±ìŠ¤ ì„¤ì •
faiss_index_file = os.getenv("FAISS_INDEX_FILE")
faiss_metadata_file = os.getenv("FAISS_META_FILE")
os.makedirs("faiss", exist_ok=True)

embedding_dim = 1536
if os.path.exists(faiss_index_file):
    index = faiss.read_index(faiss_index_file)
    with open(faiss_metadata_file, "rb") as f:
        metadata_list = pickle.load(f)
else:
    index = faiss.IndexFlatL2(embedding_dim)
    metadata_list = []

# 5. JSON â†’ ìì—°ì–´ ë³‘ë ¬ ë¬¸ì¥ ë³€í™˜ í•¨ìˆ˜
def individual_json_to_text(data: dict) -> str:
    type_name = data.get("type", "Site")
    ind_id = data.get("id", "(unknown)")
    name = data.get("name", "(no name)")
    address = data.get("address", "ì£¼ì†Œ ì •ë³´ ì—†ìŒ")
    city = data.get("city", "ë„ì‹œ ì •ë³´ ì—†ìŒ")
    country = data.get("country", "êµ­ê°€ ì •ë³´ ì—†ìŒ")
    lat = data.get("latitude", "ìœ„ë„ ì—†ìŒ")
    lon = data.get("longitude", "ê²½ë„ ì—†ìŒ")

    # ì˜ì–´ ë¬¸ì¥
    eng = (
        f"{ind_id} is an individual of type {type_name}. "
        f"Its name is {name}. "
        f"It is located in {city}, at {address}, with latitude {lat}Â° and longitude {lon}Â° in {country}."
    )

    # í•œê¸€ ë¬¸ì¥
    kor = (
        f"{ind_id}ëŠ” {type_name} íƒ€ì…ì˜ ê°œì²´ì…ë‹ˆë‹¤. ì´ë¦„ì€ {name}ì´ë©°, "
        f"{country} {city}ì˜ {address}ì— ìœ„ì¹˜í•´ ìˆê³ , "
        f"ìœ„ë„ëŠ” {lat}Â°, ê²½ë„ëŠ” {lon}Â°ì…ë‹ˆë‹¤."
    )

    return f"{kor} / {eng}"


# 6. ì„ë² ë”© í•¨ìˆ˜
def embed_text(text: str):
    try:
        response = client.embeddings.create(
            input=text,
            model=os.getenv("EMBEDDING_MODEL")
            # "text-embedding-3-small"
        )
        return response.data[0].embedding
    except Exception as e:
        logging.error(f"âŒ OpenAI embedding error: {e}")
        return []

# 7. ì „ì²´ ì²˜ë¦¬ í•¨ìˆ˜
def create_site_individual(data):
    logging.info(f"âœ… Received: {json.dumps(data, ensure_ascii=False)}")
    text = individual_json_to_text(data)
    print(text)
    logging.info(f"ğŸ“ Converted to text: {text}")

    embedding = embed_text(text)
    if not embedding:
        return

    vector = np.array(embedding, dtype=np.float32).reshape(1, -1)

    # FAISSì— ì¶”ê°€
    index.add(vector)
    metadata_list.append({
        "text": text,
        "source": "site",
        "raw": data
    })

    # ì €ì¥
    faiss.write_index(index, faiss_index_file)
    with open(faiss_metadata_file, "wb") as f:
        pickle.dump(metadata_list, f)

    logging.info(f"ğŸ“Œ Added to FAISS. Total vectors: {index.ntotal}")

# 8. Kafka Consumer ì‹œì‘
consumer = KafkaConsumer(
    os.getenv("SITE_TOPIC_NAME"),
    bootstrap_servers=os.getenv("KAFKA_IP"),
    auto_offset_reset='latest',
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

print("ğŸ“¡ Listening on topic:", os.getenv("SITE_TOPIC_NAME"))

for message in consumer:
    site_data = message.value
    print(site_data)
    try:
        create_site_individual(site_data)
    except Exception as e:
        logging.error(f"âŒ Error in create_site_individual: {e}")
        print("âŒ ì˜¤ë¥˜ ë°œìƒ:", e)
