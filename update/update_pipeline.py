# update_pipeline.py
import logging
from datetime import datetime
from fuseki.fuseki_query import (
    get_classes,
    get_object_properties,
    get_data_properties,
    get_individuals_with_literals_and_relations,
    get_swrl_rules,
)
from utils.ontology_to_text import ontology_elements_to_sentences
from utils.ontology_to_text import triples_to_sentences
from embedding.embedding import get_embedding
from embedding.embedding import embed_sentences
from embedding.faiss_store import save_faiss_index

from utils.ontology_to_text import triples_to_natural_text
from fuseki.fuseki_query import get_triples_for_individual
from embedding.faiss_store import append_to_faiss_index

timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
log_file = f"logs/update_pipeline_{timestamp}.log"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_file, encoding="utf-8"),
        logging.StreamHandler()
    ],
)

def save_sentences_to_file(sentences, filename):
    with open(filename, "w", encoding="utf-8") as f:
        for i, s in enumerate(sentences, 1):
            f.write(f"{i}. {s}\n")

def update_single_individual_index(individual_uri: str):
    try:
        logging.info(f"ğŸ” Fusekiì—ì„œ triple ì¡°íšŒ: {individual_uri}")
        triples = get_triples_for_individual(individual_uri)

        if not triples:
            logging.warning(f"âš ï¸ tripleì´ ì—†ìŠµë‹ˆë‹¤: {individual_uri}")
            return False

        logging.info(f"ğŸ“„ triple â†’ ìì—°ì–´ ë¬¸ì¥ ë³€í™˜ ì¤‘...")
        sentences = triples_to_natural_text(triples)
        logging.info(f"ğŸ“„ ë³€í™˜ëœ ë¬¸ì¥ ìˆ˜: {len(sentences)}")

        logging.info(f"ğŸ”¢ ë¬¸ì¥ â†’ OpenAI ì„ë² ë”© ì¤‘...")
        embeddings = embed_sentences(sentences)

        logging.info(f"ğŸ§¾ ë¬¸ì¥ ìƒ˜í”Œ: {sentences[0]}")
        logging.info(f"ğŸ“ ì„ë² ë”© ê¸¸ì´: {len(embeddings[0])}, íƒ€ì…: {type(embeddings[0][0])}")

        logging.info(f"ğŸ’¾ FAISSì— ì„ë² ë”© ì¶”ê°€ ì¤‘...")
        append_to_faiss_index(sentences, embeddings)

        logging.info(f"âœ… ë‹¨ì¼ Individual ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {individual_uri}")
        return True

    except Exception as e:
        logging.error(f"âŒ ë‹¨ì¼ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        return False



def main():
    logging.info("ğŸ“¡ Fusekië¡œë¶€í„° ì˜¨í†¨ë¡œì§€ ìš”ì†Œ ìˆ˜ì§‘ ì¤‘...")
    classes = get_classes()
    obj_props = get_object_properties()
    data_props = get_data_properties()
    individuals = get_individuals_with_literals_and_relations()
    rules = get_swrl_rules()

    logging.info("ğŸ§  ìì—°ì–´ ë¬¸ì¥ ë³€í™˜ ì¤‘...")
    sentences = ontology_elements_to_sentences(classes, obj_props, data_props, individuals, rules)
    logging.info(f"âœ… ì´ {len(sentences)}ê°œ ë¬¸ì¥ ìƒì„±ë¨")
    
    sentence_file = f"logs/sentences_{timestamp}.log"
    save_sentences_to_file(sentences, sentence_file)

    embeddings = []
    valid_sentences = []
    logging.info("ğŸ§¬ ì„ë² ë”© ìƒì„± ì‹œì‘")
    for s in sentences:
        try:
            emb = get_embedding(s)
            embeddings.append(emb)
            valid_sentences.append(s)
        except Exception as e:
            logging.warning(f"ì„ë² ë”© ì‹¤íŒ¨: {s} => {e}")

    save_faiss_index(valid_sentences, embeddings)
    logging.info("âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")

if __name__ == "__main__":
    main()
