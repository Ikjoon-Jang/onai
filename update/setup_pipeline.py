# setup_pipeline.py
import os
import logging
from datetime import datetime
from dotenv import load_dotenv

from fuseki.fuseki_query import get_all_ontology_elements
from utils.ontology_to_text import ontology_elements_to_sentences
from embedding.embedding import get_embedding
from embedding.faiss_store import save_faiss_index

# ğŸ“Œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ğŸ“ ë¡œê·¸ í´ë” ì¤€ë¹„
os.makedirs("logs", exist_ok=True)
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

log_filename = f"logs/pipeline_{timestamp}.log"
sentence_log_filename = f"logs/sentences_{timestamp}.log"

# ğŸªµ ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filename, encoding="utf-8"),
        logging.StreamHandler()
    ],
)

# ğŸ§¹ ê¸°ì¡´ FAISS íŒŒì¼ ì´ˆê¸°í™”
index_file = os.getenv("FAISS_INDEX_FILE")
meta_file = os.getenv("FAISS_META_FILE")

if index_file and os.path.exists(index_file):
    os.remove(index_file)
    logging.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ íŒŒì¼ ì‚­ì œ: {index_file}")

if meta_file and os.path.exists(meta_file):
    os.remove(meta_file)
    logging.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ë©”íƒ€ë°ì´í„° íŒŒì¼ ì‚­ì œ: {meta_file}")

logging.info("ğŸ§¹ ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì •ë¦¬ ì™„ë£Œ â†’ ìƒˆë¡œìš´ ë²¡í„°ë¡œ ì¬êµ¬ì¶• ì‹œì‘")

def save_sentences_to_file(sentences, filename):
    with open(filename, "w", encoding="utf-8") as f:
        for i, s in enumerate(sentences, 1):
            f.write(f"{i}. {s}\n")
    logging.info(f"ğŸ“„ ìì—°ì–´ ë¬¸ì¥ {len(sentences)}ê°œë¥¼ '{filename}'ì— ì €ì¥ ì™„ë£Œ")

def main():
    logging.info("ğŸš€ Fusekiì—ì„œ ì˜¨í†¨ë¡œì§€ ìš”ì†Œ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    classes, object_props, data_props, individuals, rules = get_all_ontology_elements()
    logging.info("âœ… Fuseki ìš”ì†Œ ë¡œë”© ì™„ë£Œ")

    logging.info("ğŸ§  ìì—°ì–´ ë¬¸ì¥ ë³€í™˜ ì‹œì‘...")
    sentences = ontology_elements_to_sentences(classes, object_props, data_props, individuals, rules)
    save_sentences_to_file(sentences, sentence_log_filename)
    logging.info(f"âœ… ì´ {len(sentences)}ê°œ ë¬¸ì¥ ìƒì„± ì™„ë£Œ")

    logging.info("ğŸ” OpenAI ì„ë² ë”© ìˆ˜í–‰ ì¤‘...")
    embeddings = []
    valid_sentences = []

    for sentence in sentences:
        try:
            emb = get_embedding(sentence)
            embeddings.append(emb)
            valid_sentences.append(sentence)
        except Exception as e:
            logging.warning(f"âŒ ì„ë² ë”© ì‹¤íŒ¨: '{sentence[:30]}...' â†’ {e}")

    # âœ… dict í˜•íƒœ ì˜¤ì—¼ ë°©ì§€
    embeddings = [e["embedding"] if isinstance(e, dict) else e for e in embeddings]

    logging.info(f"âœ… ì„ë² ë”© ì™„ë£Œ: {len(embeddings)}ê°œ")

    logging.info("ğŸ’¾ FAISS ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
    save_faiss_index(valid_sentences, embeddings)
    logging.info("âœ… FAISS ì €ì¥ ì™„ë£Œ")

if __name__ == "__main__":
    main()
